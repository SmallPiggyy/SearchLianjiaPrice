import requests
import parsel
import csv
from urllib.parse import quote  # å¤„ç†ä¸­æ–‡åŒºåŸŸåURLç¼–ç 
from fun import *
#åŒ—äº¬åŒºåŸŸåˆ—è¡¨ https://sh.lianjia.com/zufang/pg3rs%E4%B8%B4%E6%B8%AF/#contentList
TARGET_REGION=["ä¸´æ¸¯","å¥‰è´¤","æµ¦ä¸œ"]
# 1.2 åˆ†é¡µèŒƒå›´ï¼ˆ1-19é¡µï¼‰
PAGE_START = 1
PAGE_END = 3
pages = range(PAGE_START, PAGE_END + 1)
f = open('data.csv', mode='w', encoding='utf-8', newline='')
csv_writer = csv.DictWriter(f,fieldnames=['æ ‡é¢˜','ä»·æ ¼','åŒºåŸŸ','æˆ¿å­é¢ç§¯','æœå‘','æˆ¿å±‹ç±»å‹','æ¥¼å±‚','é“¾æ¥'])
csv_writer.writeheader()

#æ¨¡æ‹Ÿæµè§ˆå™¨
headers={
    'cookie':'',
    'referer':'https://sh.lianjia.com/',
    'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0'
}

#å‘é€è¯·æ±‚

def crawl_lingang(page,TARGET_REGION):
    """
    çˆ¬å–ä¸´æ¸¯åŒºåŸŸå•ä¸ªé¡µç çš„ç§Ÿæˆ¿æ•°æ®
    :param page: é¡µç ï¼ˆ1-19ï¼‰
    """
    # å¤„ç†â€œä¸´æ¸¯â€çš„URLç¼–ç ï¼ˆé¿å…ä¸­æ–‡ä¹±ç ï¼šä¸´æ¸¯ â†’ %E4%B8%B4%E6%B8%AFï¼‰

    encoded_region = quote(TARGET_REGION, encoding='utf-8')
    # æ„é€ åˆ†é¡µURLï¼ˆéµå¾ªé“¾å®¶æ ¼å¼ï¼špg+é¡µç +rs+åŒºåŸŸï¼‰
    url = f'https://sh.lianjia.com/zufang/pg{page}rs{encoded_region}/#contentList'
    try:
        # åˆå§‹åŒ–ä¼šè¯ï¼ˆä¿æŒCookieæŒä¹…åŒ–ï¼Œé™ä½éªŒè¯ç è§¦å‘æ¦‚ç‡ï¼‰
        session = requests.Session()
        session.headers.update(headers)
        # å‘é€è¯·æ±‚ï¼ˆè¶…æ—¶15ç§’ï¼Œé¿å…ç½‘ç»œå¡é¡¿å¡æ­»ï¼‰
        response = session.get(url, timeout=15)
        response.raise_for_status()  # çŠ¶æ€ç é200æ—¶æŠ›å‡ºHTTPé”™è¯¯
        # æ£€æµ‹æ˜¯å¦è§¦å‘éªŒè¯ç 
        if "äººæœºéªŒè¯" in response.text:
            print(f"âš ï¸  ä¸´æ¸¯ç¬¬{page}é¡µè§¦å‘éªŒè¯ç ï¼Œè·³è¿‡è¯¥é¡µï¼")
            return
        # è§£æHTML
        selector = parsel.Selector(response.text)
        # å®šä½æœ‰æ•ˆæˆ¿æºèŠ‚ç‚¹ï¼ˆè¿‡æ»¤æ— æ•ˆå…ƒç´ ï¼Œä»…ä¿ç•™å¸¦data-el="listItem"çš„æˆ¿æºï¼‰
        house_list = selector.css('div.content__list--item[data-el="listItem"]')

        if not house_list:
            print(f"â„¹ï¸  ä¸´æ¸¯ç¬¬{page}é¡µæœªæ‰¾åˆ°æˆ¿æºï¼Œå·²æ— æ›´å¤šæ•°æ®ï¼")
            return

        print(f"âœ…  å¼€å§‹çˆ¬å–ä¸´æ¸¯ç¬¬{page}é¡µï¼Œå…±{len(house_list)}æ¡æˆ¿æº")
        for house in house_list:
            dit = get_house_info(house)
            csv_writer.writerow(dit)  #å†™å…¥æ•°æ®
            print(dit)
    except:
        print("è¯·æ±‚å¤±è´¥æˆ–è§¦å‘éªŒè¯ç ï¼Œå°è¯•ç‚¹å‡»éªŒè¯ç ï¼")
        print(f"çŠ¶æ€ç ï¼š{response.status_code}")
        print("å“åº”å†…å®¹ï¼š", response.text[:500])  # æ‰“å°éƒ¨åˆ†å“åº”å†…å®¹ç”¨äºè°ƒè¯•


if __name__ == "__main__":
    print("ğŸš€  å¼€å§‹ä¸´æ¸¯åŒºåŸŸ1-19é¡µç§Ÿæˆ¿æ•°æ®çˆ¬å–ä»»åŠ¡ï¼")
    # å¾ªç¯çˆ¬å–1åˆ°19é¡µ
    for page in pages:
        crawl_lingang(page,TARGET_REGION[0])
    print(f"ğŸ‰  ä¸´æ¸¯åŒºåŸŸ{page}é¡µçˆ¬å–ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼æ•°æ®å·²ä¿å­˜è‡³ lingang_rent_data.csv")

