import requests
import parsel
import csv
import time
import random
from urllib.parse import quote  # å¤„ç†ä¸­æ–‡åŒºåŸŸåURLç¼–ç 

# -------------------------- 1. åŸºç¡€é…ç½® --------------------------
# 1.1 ç›®æ ‡åŒºåŸŸï¼ˆå›ºå®šä¸ºâ€œä¸´æ¸¯â€ï¼‰
TARGET_REGION = "ä¸´æ¸¯"
# 1.2 åˆ†é¡µèŒƒå›´ï¼ˆ1-19é¡µï¼‰
PAGE_START = 1
PAGE_END = 19
# 1.3 è¯·æ±‚å¤´ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼Œä¿æŒCookieæœ‰æ•ˆæ€§ï¼‰
HEADERS = {
    'cookie': 'lianjia_uuid=e8418c57-bd1b-432f-89ec-c44a63b81444; select_city=310000; crosSdkDT2019DeviceId=-vpowgi--t2f7fy-ixcq06uypztkpmy-pfii0dau1; lianjia_ssid=f8289001-1e7b-47d3-85a7-da56c54876cf; Hm_lvt_46bf127ac9b856df503ec2dbf942b67e=1763354894; HMACCOUNT=8FF0B3D99D026356; _jzqc=1; _jzqx=1.1763354896.1763354896.1.jzqsr=bing%2Ecom|jzqct=/.-; _jzqckmp=1; _ga=GA1.2.1344929102.1763354911; _gid=GA1.2.671216030.1763354911; hip=iLLlmtmVUulqQB2CgDfjPjn__iLH0QRXv2NY3OoyrHeT8FNjYXRzGCRfc5az1Fi7RPAx60Z7h0EU5Rj4_Ncc8gMePZQOr0mHouYrGbCEEwmYZyEgJZNQn25uluDqWo7CuQxO4qO4fPAYE1n21BCvtkCONOgXXUwrr0BKtZjpivCTNh72QHxxTJr459-tF6FHydP0ojmx-CKzo7ypPxQ79QllP7K3TrgFZTdY5VaqdVWr2hEw_SU0LUQOX79TISLxB-ndlQ%3D%3D; login_ucid=2000000513410548; lianjia_token=2.001326377848175f60028b1e49b7ac454a; lianjia_token_secure=2.001326377848175f60028b1e49b7ac454a; security_ticket=KpSEG9G0qBKNelBGhN5RxI++UNNiKtn3G/OMTr3w3sL4xi3y8YBiEEHKmY8Ja3+hcoIy+7Me0nSpTH95nsIko5I3XpbbJakXhlI4YW/vP7A2mRQuWQrEYXAkTVf6pns+UKykUmO4N76kYZBEH6RzWWOWk8mYWTgb+JaJ52UlX80=; ftkrc_=65b3bd39-1b00-4ae0-8648-601582529b99; lfrc_=f9ee1408-7ecd-49e5-b74a-702032ae48a6; session_id=c48a01a8-d798-0929-8aa6-36ff63e426f2; beikeBaseData=%7B%22parentSceneId%22%3A%22%22%7D; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%2219a640ec95dad6-0bf3b8b8603e2c8-4c657b58-2710825-19a640ec95ee89%22%2C%22%24device_id%22%3A%2219a640ec95dad6-0bf3b8b8603e2c8-4c657b58-2710825-19a640ec95ee89%22%2C%22props%22%3A%7B%22%24latest_traffic_source_type%22%3A%22%E7%9B%B4%E6%8E%A5%E6%B5%81%E9%87%8F%22%2C%22%24latest_referrer%22%3A%22%22%2C%22%24latest_referrer_host%22%3A%22%22%2C%22%24latest_search_keyword%22%3A%22%E6%9C%AA%E5%8F%96%E5%88%B0%E5%80%BC_%E7%9B%B4%E6%8E%A5%E6%89%93%E5%BC%80%22%2C%22%24latest_utm_source%22%3A%22biying%22%2C%22%24latest_utm_medium%22%3A%22pinzhuan%22%2C%22%24latest_utm_campaign%22%3A%22wybeijing%22%2C%22%24latest_utm_content%22%3A%22biaotimiaoshu%22%2C%22%24latest_utm_term%22%3A%22biaoti%22%7D%7D; Hm_lpvt_46bf127ac9b856df503ec2dbf942b67e=1763357722; _jzqa=1.1208091033271271200.1763354896.1763354896.1763357723.2; _jzqb=1.1.10.1763357723.1; _ga_GVYN2J1PCG=GS2.2.s1763357734$o2$g0$t1763357734$j60$l0$h0; _ga_LRLL77SF11=GS2.2.s1763357734$o2$g0$t1763357734$j60$l0$h0',
    'referer': 'https://sh.lianjia.com/',
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0'
}
# 1.4 åˆå§‹åŒ–CSVæ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼Œé¿å…è¦†ç›–æ•°æ®ï¼‰
with open('lingang_rent_data.csv', mode='a', encoding='utf-8', newline='') as f:
    csv_writer = csv.DictWriter(f, fieldnames=[
        'é¡µç ', 'æ ‡é¢˜', 'ä»·æ ¼', 'åŒºåŸŸ', 'æˆ¿å­é¢ç§¯', 'æœå‘', 'æˆ¿å±‹ç±»å‹', 'æ¥¼å±‚', 'é“¾æ¥', 'çˆ¬å–æ—¶é—´'
    ])
    # ä»…åœ¨æ–‡ä»¶ä¸ºç©ºæ—¶å†™å…¥è¡¨å¤´ï¼ˆé˜²æ­¢é‡å¤ï¼‰
    if f.tell() == 0:
        csv_writer.writeheader()
block_page=[]

# -------------------------- 2. æ ¸å¿ƒçˆ¬å–å‡½æ•° --------------------------
def crawl_lingang(page):
    """
    çˆ¬å–ä¸´æ¸¯åŒºåŸŸå•ä¸ªé¡µç çš„ç§Ÿæˆ¿æ•°æ®
    :param page: é¡µç ï¼ˆ1-19ï¼‰
    """
    # å¤„ç†â€œä¸´æ¸¯â€çš„URLç¼–ç ï¼ˆé¿å…ä¸­æ–‡ä¹±ç ï¼šä¸´æ¸¯ â†’ %E4%B8%B4%E6%B8%AFï¼‰
    encoded_region = quote(TARGET_REGION, encoding='utf-8')
    # æ„é€ åˆ†é¡µURLï¼ˆéµå¾ªé“¾å®¶æ ¼å¼ï¼špg+é¡µç +rs+åŒºåŸŸï¼‰
    url = f'https://sh.lianjia.com/zufang/pg{page}rs{encoded_region}/#contentList'

    try:
        # åˆå§‹åŒ–ä¼šè¯ï¼ˆä¿æŒCookieæŒä¹…åŒ–ï¼Œé™ä½éªŒè¯ç è§¦å‘æ¦‚ç‡ï¼‰
        session = requests.Session()
        session.headers.update(HEADERS)

        # å‘é€è¯·æ±‚ï¼ˆè¶…æ—¶15ç§’ï¼Œé¿å…ç½‘ç»œå¡é¡¿å¡æ­»ï¼‰
        response = session.get(url, timeout=15)
        response.raise_for_status()  # çŠ¶æ€ç é200æ—¶æŠ›å‡ºHTTPé”™è¯¯

        # æ£€æµ‹æ˜¯å¦è§¦å‘éªŒè¯ç 
        if "äººæœºéªŒè¯" in response.text:

            print(f"âš ï¸  ä¸´æ¸¯ç¬¬{page}é¡µè§¦å‘éªŒè¯ç ï¼Œè·³è¿‡è¯¥é¡µï¼")
            return

        # è§£æHTML
        selector = parsel.Selector(response.text)
        # å®šä½æœ‰æ•ˆæˆ¿æºèŠ‚ç‚¹ï¼ˆè¿‡æ»¤æ— æ•ˆå…ƒç´ ï¼Œä»…ä¿ç•™å¸¦data-el="listItem"çš„æˆ¿æºï¼‰
        house_list = selector.css('div.content__list--item[data-el="listItem"]')

        if not house_list:
            print(f"â„¹ï¸  ä¸´æ¸¯ç¬¬{page}é¡µæœªæ‰¾åˆ°æˆ¿æºï¼Œå·²æ— æ›´å¤šæ•°æ®ï¼")
            return

        print(f"âœ…  å¼€å§‹çˆ¬å–ä¸´æ¸¯ç¬¬{page}é¡µï¼Œå…±{len(house_list)}æ¡æˆ¿æº")

        # éå†è§£ææ¯æ¡æˆ¿æº
        for house in house_list:
            # 2.1 åŸºç¡€ä¿¡æ¯æå–
            # æ ‡é¢˜ï¼ˆå¦‚â€œæ•´ç§ŸÂ·ç»¿åœ°ä¸œå²¸æ¶ŸåŸ 5å®¤1å… å—/åŒ—â€ï¼‰
            title = house.css('p.content__list--item--title a::text').get()
            title = title.strip() if title else 'æ— æ ‡é¢˜'
            # ä»·æ ¼ï¼ˆå¦‚â€œ8900 å…ƒ/æœˆâ€ï¼‰
            price = house.css('span.content__list--item-price em::text').get()
            price = f'{price} å…ƒ/æœˆ' if price else 'æ— ä»·æ ¼'
            # åŒºåŸŸè¯¦æƒ…ï¼ˆæµ¦ä¸œ-ä¸´æ¸¯æ–°åŸ-å°åŒºåï¼‰
            area_info = house.css('p.content__list--item--des a::text').getall()
            area = '-'.join(area_info) if area_info else 'æ— åŒºåŸŸä¿¡æ¯'
            # æˆ¿æºé“¾æ¥ï¼ˆè¡¥å…¨ç»å¯¹è·¯å¾„ï¼‰
            link = house.css('a.content__list--item--aside::attr(href)').get()
            link = f'https://sh.lianjia.com{link}' if link else 'æ— é“¾æ¥'
            # çˆ¬å–æ—¶é—´ï¼ˆä¾¿äºæ•°æ®è¿½æº¯ï¼‰
            crawl_time = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
            # 2.2 æˆ¿å±‹è¯¦æƒ…æå–ï¼ˆé¢ç§¯ã€æœå‘ã€æˆ·å‹ã€æ¥¼å±‚ï¼‰
            # è·å–<p>æ ‡ç­¾ä¸‹æ‰€æœ‰æ–‡æœ¬ï¼ˆåŒ…æ‹¬éšè—çš„æ¥¼å±‚ä¿¡æ¯ï¼‰
            all_details = house.css('p.content__list--item--des *::text').getall()
            # æ¸…æ´—æ–‡æœ¬ï¼šå»é™¤ç©ºå€¼ã€â€œ/â€åˆ†éš”ç¬¦ã€å¤šä½™ç©ºæ ¼
            clean_details = []
            for d in all_details:
                d_clean = d.strip()
                if d_clean and d_clean != '/':
                    clean_details.append(d_clean)
            # æ’é™¤å·²æå–çš„åŒºåŸŸä¿¡æ¯ï¼ˆé¿å…é‡å¤è§£æï¼‰
            house_details = clean_details[len(area_info):] if area_info else clean_details

            # æŒ‰å…³é”®è¯åˆ†ç±»æå–è¯¦æƒ…ï¼ˆé¿å…é¡ºåºé”™ä½ï¼‰
            area_size = 'æ— é¢ç§¯'  # å«â€œã¡â€
            direction = 'æ— æœå‘'  # å«â€œä¸œ/å—/è¥¿/åŒ—â€
            layout = 'æ— æˆ·å‹'  # å«â€œå®¤/å…/å«â€
            floor = 'æ— æ¥¼å±‚'  # å«â€œæ¥¼å±‚â€

            for detail in house_details:
                if 'ã¡' in detail:
                    area_size = detail
                elif any(dir_key in detail for dir_key in ['ä¸œ', 'å—', 'è¥¿', 'åŒ—']):
                    direction = detail
                elif any(layout_key in detail for layout_key in ['å®¤', 'å…', 'å«']):
                    layout = detail
                elif 'æ¥¼å±‚' in detail:
                    floor = detail  # åŒ¹é…â€œé«˜æ¥¼å±‚ï¼ˆ14å±‚ï¼‰â€æ ¼å¼

            # 2.3 ä¿å­˜æ•°æ®åˆ°CSV
            house_data = {
                'é¡µç ': page,
                'æ ‡é¢˜': title,
                'ä»·æ ¼': price,
                'åŒºåŸŸ': area,
                'æˆ¿å­é¢ç§¯': area_size,
                'æœå‘': direction,
                'æˆ¿å±‹ç±»å‹': layout,
                'æ¥¼å±‚': floor,
                'é“¾æ¥': link,
                'çˆ¬å–æ—¶é—´': crawl_time
            }

            # è¿½åŠ å†™å…¥CSV
            with open('lingang_rent_data.csv', mode='a', encoding='utf-8', newline='') as f:
                csv_writer = csv.DictWriter(f, fieldnames=house_data.keys())
                csv_writer.writerow(house_data)

            # æ‰“å°å•æ¡æ•°æ®ï¼ˆå¯é€‰ï¼Œç”¨äºå®æ—¶æŸ¥çœ‹è¿›åº¦ï¼‰
            print(f"ğŸ“„  ä¿å­˜ï¼š{title} | {price} | {area}")

        # éšæœºç¡çœ 1-3ç§’ï¼ˆæ¨¡æ‹Ÿäººç±»æµè§ˆï¼Œé™ä½åçˆ¬æ£€æµ‹ï¼‰
        sleep_time = random.uniform(1, 3)
        print(f"ğŸ’¤  ä¸´æ¸¯ç¬¬{page}é¡µçˆ¬å–å®Œæˆï¼Œéšæœºç¡çœ {sleep_time:.2f}ç§’...\n")
        time.sleep(sleep_time)

    except requests.exceptions.HTTPError as e:
        print(f"âŒ  ä¸´æ¸¯ç¬¬{page}é¡µHTTPé”™è¯¯ï¼š{e}")
    except requests.exceptions.Timeout as e:
        print(f"âŒ  ä¸´æ¸¯ç¬¬{page}é¡µè¯·æ±‚è¶…æ—¶ï¼š{e}")
    except Exception as e:
        print(f"âŒ  ä¸´æ¸¯ç¬¬{page}é¡µæœªçŸ¥é”™è¯¯ï¼š{str(e)}")


# -------------------------- 3. æ‰¹é‡çˆ¬å–1-19é¡µ --------------------------


block_pages=[10,13,15,18]
if __name__ == "__main__":
    print("ğŸš€  å¼€å§‹ä¸´æ¸¯åŒºåŸŸ1-19é¡µç§Ÿæˆ¿æ•°æ®çˆ¬å–ä»»åŠ¡ï¼")
    # å¾ªç¯çˆ¬å–1åˆ°19é¡µ
    for block_pages in block_pages:
        crawl_lingang(block_pages)
    print(f"ğŸ‰  ä¸´æ¸¯åŒºåŸŸ{block_page}é¡µçˆ¬å–ä»»åŠ¡å…¨éƒ¨å®Œæˆï¼æ•°æ®å·²ä¿å­˜è‡³ lingang_rent_data.csv")

